{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T16Q7rZrnhC3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3my2rb2x5Dfi",
        "outputId": "7ef0e72a-f116-4d61-9b5a-b3fc859fff9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K6JZPb_J6MTB"
      },
      "outputs": [],
      "source": [
        "!mkdir -p datasets/flores101\n",
        "!mkdir -p datasets/in22\n",
        "!mkdir -p models\n",
        "!mkdir -p translations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MZXvKm6T6Qzz"
      },
      "outputs": [],
      "source": [
        "!mkdir -p '/content/drive/My Drive/IndicTrans2/datasets'\n",
        "!mkdir -p '/content/drive/My Drive/IndicTrans2/models'\n",
        "!mkdir -p '/content/drive/My Drive/IndicTrans2/translations'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjuk-2OI6TjQ",
        "outputId": "90285edc-954e-4dc4-b9b2-9afaee82e014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'flores' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/flores.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpF4lq076XSH",
        "outputId": "482b1a22-88a2-4865-f0a5-ca3c710077b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/flores\n",
            "bash: scripts/download_flores101_dataset.sh: No such file or directory\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd flores\n",
        "!bash scripts/download_flores101_dataset.sh\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig7XEunM6ahJ",
        "outputId": "2a655fde-ae27-4ed9-f1db-b963388b8887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-27 17:18:46--  https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/translation/indicat/IN22/IN22-Gen.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.117.207, 142.250.99.207, 142.250.107.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.117.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-11-27 17:18:47 ERROR 404: Not Found.\n",
            "\n",
            "--2024-11-27 17:18:47--  https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/translation/indicat/IN22/IN22-Conv.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.117.207, 142.250.99.207, 142.250.107.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.117.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-11-27 17:18:48 ERROR 404: Not Found.\n",
            "\n",
            "unzip:  cannot find or open datasets/in22/IN22-Gen.zip, datasets/in22/IN22-Gen.zip.zip or datasets/in22/IN22-Gen.zip.ZIP.\n",
            "unzip:  cannot find or open datasets/in22/IN22-Conv.zip, datasets/in22/IN22-Conv.zip.zip or datasets/in22/IN22-Conv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!wget -P datasets/in22/ https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/translation/indicat/IN22/IN22-Gen.zip\n",
        "!wget -P datasets/in22/ https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/translation/indicat/IN22/IN22-Conv.zip\n",
        "\n",
        "!unzip datasets/in22/IN22-Gen.zip -d datasets/in22/\n",
        "!unzip datasets/in22/IN22-Conv.zip -d datasets/in22/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X6PKkK06j8h"
      },
      "source": [
        "datasets/\n",
        "├── flores101/\n",
        "│   ├── dev/\n",
        "│   ├── devtest/\n",
        "├── in22/\n",
        "    ├── IN22-Gen/\n",
        "    ├── IN22-Conv/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9W1epL_6cZt",
        "outputId": "0d10fcec-89df-4fe6-a018-021d6369f182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
            "Requirement already satisfied: transformers==4.24.0 in /usr/local/lib/python3.10/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2024.8.30)\n",
            "Collecting fairseq==0.12.2\n",
            "  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.4 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/92/b1/4f3023143436f12c98bab53f0b3db617bd18a7d223627d5030e13a7b4fc2/omegaconf-2.0.4-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.3 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/29/08/a88210c2c1aa0a3f65f05d8a6c98939ccb84b6fb982aa6567dec4e6773f9/omegaconf-2.0.3-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.2 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/72/fe/f8d162aa059fb4f327fd75144dd69aa7e8acbb6d8d37013e4638c8490e0b/omegaconf-2.0.2-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.1 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/86/ec/605805e60abdb025b06664d107335031bb8ebdc52e0a90bdbad6a7130279/omegaconf-2.0.1-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.26.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.9.11)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.6)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Using cached bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.1+cu121)\n",
            "Collecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached omegaconf-2.0.5-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[33mWARNING: Ignoring version 2.0.5 of omegaconf since it has invalid metadata:\n",
            "Requested omegaconf<2.1 from https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl (from fairseq==0.12.2) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    PyYAML (>=5.1.*)\n",
            "            ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0mINFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install fairseq and fairseq==0.12.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    fairseq 0.12.2 depends on omegaconf<2.1\n",
            "    hydra-core 1.0.7 depends on omegaconf<2.1 and >=2.0.5\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install sacremoses sentencepiece\n",
        "!pip install sacrebleu\n",
        "!pip install transformers==4.24.0\n",
        "!pip install fairseq==0.12.2\n",
        "!pip install omegaconf\n",
        "!pip install pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-rDxuAk6kt1",
        "outputId": "489a6c7e-010c-44e8-d7dc-ded0782427d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'IndicTrans2' already exists and is not an empty directory.\n",
            "/content/IndicTrans2\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git\n",
        "%cd IndicTrans2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI_FAWyiM8KQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS7qY0yvEk8M",
        "outputId": "cc774a6e-e333-4fdb-c594-0860eb730ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1jx2TwBQmUk"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODWfn4ptnhC-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets joblib openai tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qkZUWdDnhC-",
        "outputId": "ccd90bef-ac04-47cc-eed5-a61ea1eb9c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/151 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 10/151 [00:19<04:31,  1.92s/it]\u001b[A\n",
            " 10%|▉         | 15/151 [00:45<07:27,  3.29s/it]\u001b[A\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from joblib import Parallel, delayed\n",
        "from openai import APIStatusError, OpenAI\n",
        "\n",
        "# Constants and Language Mappings\n",
        "_TEMPERATURE = 0\n",
        "_MAX_TOKENS = 16384\n",
        "_TOP_P = 1\n",
        "_TOP_K = 64\n",
        "_FREQUENCY_PENALTY = 0\n",
        "_PRESENCE_PENALTY = 0\n",
        "\n",
        "_FLORES_MAP = {\n",
        "    \"Assamese\": \"asm_Beng\",\n",
        "    \"Bengali\": \"ben_Beng\",\n",
        "    \"Gujarati\": \"guj_Gujr\",\n",
        "    \"Hindi\": \"hin_Deva\",\n",
        "    \"Kannada\": \"kan_Knda\",\n",
        "    \"Malayalam\": \"mal_Mlym\",\n",
        "    \"Marathi\": \"mar_Deva\",\n",
        "    \"Nepali\": \"npi_Deva\",\n",
        "    \"Odia\": \"ory_Orya\",\n",
        "    \"Punjabi\": \"pan_Guru\",\n",
        "    \"Sanskrit\": \"san_Deva\",\n",
        "    \"Tamil\": \"tam_Taml\",\n",
        "    \"Telugu\": \"tel_Telu\",\n",
        "    \"Urdu\": \"urd_Arab\",\n",
        "    \"Bodo\": \"brx_Deva\",\n",
        "    \"Dogri\": \"doi_Deva\",\n",
        "    \"Konkani\": \"gom_Deva\",\n",
        "    \"Kashmiri\": \"kas_Arab\",\n",
        "    \"Maithili\": \"mai_Deva\",\n",
        "    \"Manipuri\": \"mni_Mtei\",\n",
        "    \"Santali\": \"sat_Olck\",\n",
        "    \"Sindhi\": \"snd_Deva\"\n",
        "}\n",
        "\n",
        "_LANG_MAP = {\n",
        "    \"asm\": \"Assamese\",\n",
        "    \"ben\": \"Bengali\",\n",
        "    \"guj\": \"Gujarati\",\n",
        "    \"hin\": \"Hindi\",\n",
        "    \"kan\": \"Kannada\",\n",
        "    \"mal\": \"Malayalam\",\n",
        "    \"mar\": \"Marathi\",\n",
        "    \"nep\": \"Nepali\",\n",
        "    \"ory\": \"Odia\",\n",
        "    \"pan\": \"Punjabi\",\n",
        "    \"san\": \"Sanskrit\",\n",
        "    \"tam\": \"Tamil\",\n",
        "    \"tel\": \"Telugu\",\n",
        "    \"urd\": \"Urdu\",\n",
        "    \"brx\": \"Bodo\",\n",
        "    \"doi\": \"Dogri\",\n",
        "    \"gom\": \"Konkani\",\n",
        "    \"kas\": \"Kashmiri\",\n",
        "    \"mai\": \"Maithili\",\n",
        "    \"mni\": \"Manipuri\",\n",
        "    \"sat\": \"Santali\",\n",
        "    \"snd\": \"Sindhi\"\n",
        "}\n",
        "\n",
        "# Define the translation function\n",
        "def translate_batch(system_prompt, task_prompt, texts, lang):\n",
        "    \"\"\"\n",
        "    Translate a batch of texts using the OpenAI API.\n",
        "    \"\"\"\n",
        "    # Initialize the OpenAI client inside the function to avoid pickling issues\n",
        "    llama = OpenAI(\n",
        "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "        api_key=\"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\"  # Replace with your actual API key\n",
        "    )\n",
        "\n",
        "    # Construct the input prompt with numbered sentences\n",
        "    _TRANSLATE_MODEL_INPUT = f\"{task_prompt}\\n\\n\"\n",
        "    for idx, text in enumerate(texts, 1):\n",
        "        _TRANSLATE_MODEL_INPUT += f\"{idx}. {text}\\n\"\n",
        "    _TRANSLATE_MODEL_INPUT += \"\\nTranslations:\\n\"\n",
        "\n",
        "    try:\n",
        "        response = llama.chat.completions.create(\n",
        "            model=\"meta/llama-3.1-405b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": _TRANSLATE_MODEL_INPUT}\n",
        "            ],\n",
        "            temperature=_TEMPERATURE,\n",
        "            max_tokens=_MAX_TOKENS,\n",
        "            top_p=_TOP_P,\n",
        "            frequency_penalty=_FREQUENCY_PENALTY,\n",
        "            presence_penalty=_PRESENCE_PENALTY\n",
        "        )\n",
        "        translations_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Parse the translations into a list\n",
        "        translations = []\n",
        "        for line in translations_text.split('\\n'):\n",
        "            if line.strip():\n",
        "                # Assuming format: \"1. [translation]\"\n",
        "                parts = line.strip().split('.', 1)\n",
        "                if len(parts) == 2:\n",
        "                    translation = parts[1].strip()\n",
        "                    translations.append(translation)\n",
        "\n",
        "        # Ensure that we have the same number of translations as inputs\n",
        "        if len(translations) != len(texts):\n",
        "            print(f\"Warning: Expected {len(texts)} translations, but got {len(translations)}.\")\n",
        "            # Fill missing translations with empty strings\n",
        "            translations += [\"\"] * (len(texts) - len(translations))\n",
        "\n",
        "        return translations\n",
        "    except APIStatusError as e:\n",
        "        if e.status_code in {402, 401}:\n",
        "            print(\"Issue with the API key or payment, skipping these translations...\")\n",
        "            return [\"\"] * len(texts)\n",
        "        else:\n",
        "            print(f\"APIStatusError: {e}\")\n",
        "            return [\"\"] * len(texts)\n",
        "    except Exception as e:\n",
        "        print(f\"Generation error encountered: {e}\")\n",
        "        return [\"\"] * len(texts)\n",
        "\n",
        "# Define the batch processing function\n",
        "def process_translation_batch(batch):\n",
        "    \"\"\"\n",
        "    Process a single batch of translations.\n",
        "    Each batch contains up to 10 rows.\n",
        "    \"\"\"\n",
        "    batch_index, rows, lang = batch\n",
        "    target_lang = _LANG_MAP.get(lang, \"Unknown\")\n",
        "\n",
        "    _MT_SYSTEM_PROMPT = f\"You are a machine translation model. Please help in translating the following text to {target_lang}.\"\n",
        "    _MT_TASK_PROMPT = (\n",
        "        f\"Translate the following English text to {target_lang}. Follow these guidelines:\\n\\n\"\n",
        "        \"1. **Accuracy:** Translate the text as accurately as possible, ensuring no information is missed.\\n\"\n",
        "        \"2. **Tone and Naturalness:** Maintain the original tone and ensure the translation sounds natural.\\n\"\n",
        "        \"3. **Meaningful Restructuring:** Do not translate word-for-word. Restructure sentences if needed.\\n\"\n",
        "        \"4. **Do not Translate the numericals.\\n\"\n",
        "        \"5. **Do not provide any explanations.\\n\\n\"\n",
        "        \"Ensure that the translated text is clear, coherent, and grammatically correct.\\n\\n\"\n",
        "    )\n",
        "\n",
        "    # Extract English sentences from the batch\n",
        "    texts = [row['sentence_eng_Latn'] for row in rows]\n",
        "\n",
        "    # Perform batch translation\n",
        "    translated_texts = translate_batch(_MT_SYSTEM_PROMPT, _MT_TASK_PROMPT, texts, lang)\n",
        "\n",
        "    # Define the output directory (ensure it's mounted on Google Drive)\n",
        "    output_dir = f\"/content/drive/MyDrive/{lang}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save each translation in its respective JSONL file\n",
        "    for j, translated_text in enumerate(translated_texts):\n",
        "        row = rows[j]\n",
        "        output_data = {\n",
        "            'source_lang': 'eng_Latn',\n",
        "            'target_lang': _FLORES_MAP.get(_LANG_MAP.get(lang, \"\"), \"unknown\"),\n",
        "            'meta': {\n",
        "                'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32',\n",
        "                'temperature': _TEMPERATURE,\n",
        "                'max_tokens': _MAX_TOKENS,\n",
        "                'top_p': _TOP_P,\n",
        "                'frequency_penalty': _FREQUENCY_PENALTY,\n",
        "                'presence_penalty': _PRESENCE_PENALTY\n",
        "            },\n",
        "            'Input Prompt': row['sentence_eng_Latn'],\n",
        "            'translated_text': translated_text,\n",
        "            'id': int(row['id']),\n",
        "        }\n",
        "        # Optional: Remove or comment out to reduce output clutter\n",
        "        # print(output_data)\n",
        "\n",
        "        # Save to JSONL\n",
        "        with open(f\"{output_dir}/{lang}_translations_{batch_index + j}.jsonl\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, ensure_ascii=False)\n",
        "\n",
        "# Function to create batches\n",
        "def create_batches(df, batch_size=10):\n",
        "    \"\"\"\n",
        "    Split the DataFrame into batches of specified size.\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    total = len(df)\n",
        "    for i in range(0, total, batch_size):\n",
        "        batch_rows = df.iloc[i:i+batch_size].to_dict(orient='records')\n",
        "        batches.append( (i, batch_rows, lang) )\n",
        "    return batches\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Google Drive (if not already mounted)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load the dataset for English to Marathi translation\n",
        "    in22_dataset = load_dataset(\"ai4bharat/IN22-Conv\", \"eng_Latn-mar_Deva\")\n",
        "    df = pd.DataFrame(in22_dataset[\"conv\"])\n",
        "    lang = \"mar\"  # Set your target language code here\n",
        "\n",
        "    # Create batches of 10 sentences each\n",
        "    batch_size = 10\n",
        "    batches = create_batches(df, batch_size=batch_size)\n",
        "\n",
        "    # Set the number of parallel jobs (adjust based on CPU and API rate limits)\n",
        "    num_jobs = 5  # Adjust to 5 or 10 as per your requirement\n",
        "\n",
        "    # Run translations in parallel using threading backend to avoid pickling issues\n",
        "    Parallel(n_jobs=num_jobs, backend=\"threading\")(\n",
        "        delayed(process_translation_batch)(batch) for batch in tqdm(batches)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564,
          "referenced_widgets": [
            "1b25f2b51c72418b9825dc755fd3f69f",
            "55ce8764eb5a43868dc347631da7e3fe",
            "8fbe99c86e01460eb4391e4431707044",
            "e975ce6090ed417486dd5a9dc0d282f1",
            "fb50ff166fa143a5af98f2eeb1e81be1",
            "215f6b2ac1c64db38cdf79ec670cdb1c",
            "4ba04c64cf77437185715353a18ea117",
            "78e3ab571dfb4013ab417358d3a4b12c",
            "6a43337d5e9849f380b8a9ab14673247",
            "9ace13b5690f4d2b89e304c496c680c4",
            "05dfdbbb459b4a769b8c14adbddbc459"
          ]
        },
        "collapsed": true,
        "id": "Rvn4RVQZBG2i",
        "outputId": "a8ca0e48-df37-4d38-8853-1febfa2fd02e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b25f2b51c72418b9825dc755fd3f69f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating conv split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': \"Mom, let's go for a movie tomorrow.\", 'translated_text': 'माँ, कल फिल्म देखने चलें।', 'id': 1}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': \"I don't have to go to school.\", 'translated_text': 'मुझे स्कूल जाने की ज़रूरत नहीं है।', 'id': 2}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'It is a holiday.', 'translated_text': 'आज छुट्टी है।', 'id': 3}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'Oh, tomorrow is the 14th of April right?', 'translated_text': 'अरे, कल १४ अप्रैल है ना?', 'id': 4}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'Your dad will also have the day off from work.', 'translated_text': 'आपके पापा को भी काम से छुट्टी मिलेगी।', 'id': 5}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'We can make a movie plan!', 'translated_text': 'हम एक फिल्म की योजना बना सकते हैं!', 'id': 6}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': \"That's a good news!\", 'translated_text': 'यह अच्छी खबर है!', 'id': 7}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'Why is it a holiday though?', 'translated_text': 'यह अवकाश क्यों है ?', 'id': 8}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'Are all schools, colleges and offices closed tomorrow?', 'translated_text': 'क्या कल सभी स्कूल, कॉलेज और कार्यालय बंद हैं?', 'id': 9}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'It is Ambedkar Jayanti tomorrow!', 'translated_text': 'कल अम्बेडकर जयंती है!', 'id': 10}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'This day is celebrated annually to mark the birth of Dr. B. R Ambedkar.', 'translated_text': 'इस दिन को डॉ. बी. आर. अम्बेडकर के जन्म के अवसर पर प्रतिवर्ष मनाया जाता है।', 'id': 11}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'Have you heard of him?', 'translated_text': 'क्या आपने उसके बारे में सुना है?', 'id': 12}\n",
            "/content/drive/MyDrive/hin\n",
            "{'source_lang': 'eng_Latn', 'target_lang': 'hin_Deva', 'meta': {'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32', 'temperature': 0, 'max_tokens': 16384, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}, 'Input Prompt': 'I think I have seen him in my History and Civics book.', 'translated_text': 'मुझे लगता है कि मैंने उसे अपनी इतिहास और नागरिक शास्त्र की पुस्तक में देखा है।', 'id': 13}\n"
          ]
        }
      ],
      "source": [
        "'''import os\n",
        "import json\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from joblib import Parallel, delayed\n",
        "from openai import APIStatusError, OpenAI\n",
        "\n",
        "'''parser = argparse.ArgumentParser(description=\"Run the LLM translation for Frames dataset\")\n",
        "parser.add_argument(\"--lang\", type=str, required=True, help=\"Target language for translation\")\n",
        "args = parser.parse_args()'''\n",
        "\n",
        "_TEMPERATURE = 0\n",
        "_MAX_TOKENS = 16384\n",
        "_TOP_P = 1\n",
        "_TOP_K = 64\n",
        "_FREQUENCY_PENALTY = 0\n",
        "_PRESENCE_PENALTY = 0\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-405B\",use_auth_token =True)\n",
        "\n",
        "# Language mappings\n",
        "_FLORES_MAP = {\n",
        "    \"Assamese\": \"asm_Beng\",\n",
        "    \"Bengali\": \"ben_Beng\",\n",
        "    \"Gujarati\": \"guj_Gujr\",\n",
        "    \"Hindi\": \"hin_Deva\",\n",
        "    \"Kannada\": \"kan_Knda\",\n",
        "    \"Malayalam\": \"mal_Mlym\",\n",
        "    \"Marathi\": \"mar_Deva\",\n",
        "    \"Nepali\": \"npi_Deva\",\n",
        "    \"Odia\": \"ory_Orya\",\n",
        "    \"Punjabi\": \"pan_Guru\",\n",
        "    \"Sanskrit\": \"san_Deva\",\n",
        "    \"Tamil\": \"tam_Taml\",\n",
        "    \"Telugu\": \"tel_Telu\",\n",
        "    \"Urdu\": \"urd_Arab\",\n",
        "    \"Bodo\" : \"brx_Deva\",\n",
        "    \"Dogri\":\"doi_Deva\",\n",
        "    \"Konkani\":\"gom_Deva\",\n",
        "    \"Kashmiri\":\"kas_Arab\",\n",
        "    \"Maithili\":\"mai_Deva\",\n",
        "    \"Nepali\":'npi_Deva',\n",
        "    \"Manipuri\": 'mni_Mtei',\n",
        "    \"Santali\":'sat_Olck',\n",
        "    \"Sindhi\":'snd_Deva'\n",
        "}\n",
        "\n",
        "_LANG_MAP = {\n",
        "    \"asm\": \"Assamese\",\n",
        "    \"ben\": \"Bengali\",\n",
        "    \"guj\": \"Gujarati\",\n",
        "    \"hin\": \"Hindi\",\n",
        "    \"kan\": \"Kannada\",\n",
        "    \"mal\": \"Malayalam\",\n",
        "    \"mar\": \"Marathi\",\n",
        "    \"nep\": \"Nepali\",\n",
        "    \"ory\": \"Odia\",\n",
        "    \"pan\": \"Punjabi\",\n",
        "    \"san\": \"Sanskrit\",\n",
        "    \"tam\": \"Tamil\",\n",
        "    \"tel\": \"Telugu\",\n",
        "    \"urd\": \"Urdu\",\n",
        "    \"brx\":\"Bodo\",\n",
        "    \"doi\":\"Dogri\",\n",
        "    \"gom\":\"Konkani\",\n",
        "    \"kas\":\"Kashmiri\",\n",
        "    \"mai\":\"Maithili\",\n",
        "    \"npi\":\"Nepali\",\n",
        "    \"mni\":\"Manipuri\",\n",
        "    \"sat\":\"Santali\",\n",
        "    \"snd\":\"Sindhi\"\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# Load Frames dataset\n",
        "\n",
        "\n",
        "def translate_text(llama, system_prompt, task_prompt, text):\n",
        "    \"\"\"Helper function to make a translation API call for a single text input.\"\"\"\n",
        "    _TRANSLATE_MODEL_INPUT = (\n",
        "        f\"{task_prompt}\"\n",
        "        f\"Input Text: {text}\\n\"\n",
        "        f\"Translated Text: \\n\"\n",
        "    )\n",
        "    # print(_TRANSLATE_MODEL_INPUT)\n",
        "    try:\n",
        "        response = llama.chat.completions.create(\n",
        "            #model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
        "            model=\"meta/llama-3.1-405b-instruct\",\n",
        "            messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": _TRANSLATE_MODEL_INPUT}],\n",
        "            temperature=0,\n",
        "            max_tokens=4096,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "        )\n",
        "        # print(response.choices[0].message.content)\n",
        "        return response.choices[0].message.content\n",
        "    except APIStatusError as e:\n",
        "        if e.status_code in {402, 401}:\n",
        "            print(\"Issue with the key, skipping...\")\n",
        "            return \"\"\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(\"Generation error encountered:\", e)\n",
        "        return \"\"\n",
        "\n",
        "def process_translation(i, row):\n",
        "    llama = OpenAI(\n",
        "        base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "        api_key = \"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\"\n",
        "    )\n",
        "    target_lang = _LANG_MAP[lang]\n",
        "\n",
        "    _MT_SYSTEM_PROMPT = f\"You are a machine translation model. Please help in translating the following text to {target_lang}.\"\n",
        "    _MT_TASK_PROMPT = (\n",
        "        f\"Translate the following English text to {target_lang}. Follow these guidelines:\\n\\n\"\n",
        "        \"1. **Accuracy:** Translate the text as accurately as possible, ensuring no information is missed.\\n\"\n",
        "        \"2. **Tone and Naturalness:** Maintain the original tone and ensure the translation sounds natural.\\n\"\n",
        "        \"3. **Meaningful Restructuring:** Do not translate word-for-word. Restructure sentences if needed.\\n\"\n",
        "        \"4. **Do not Translate the numericals\\n\"\n",
        "        \"5. **Do not provide any Explnations.\\n\\n\"\n",
        "        \"Ensure that the translated text is clear, coherent, and grammatically correct.\\n\\n\"\n",
        "    )\n",
        "\n",
        "    # Translate Prompt and Answer\n",
        "    translated_prompt = translate_text(llama, _MT_SYSTEM_PROMPT, _MT_TASK_PROMPT, row['sentence_eng_Latn'])\n",
        "   # translated_answer = translate_text(llama, _MT_SYSTEM_PROMPT, _MT_TASK_PROMPT, row['Answer'])\n",
        "    output_dir = f\"/content/drive/MyDrive/{lang}\"\n",
        "    #output_dir = f\"./in22-translations/{lang}\"\n",
        "    print(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_data = {\n",
        "        'source_lang': 'eng_Latn',\n",
        "        'target_lang': _FLORES_MAP[_LANG_MAP[lang]],\n",
        "        'meta': {\n",
        "            'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32',\n",
        "            'temperature': _TEMPERATURE,\n",
        "            'max_tokens': _MAX_TOKENS,\n",
        "            'top_p': _TOP_P,\n",
        "            'frequency_penalty': _FREQUENCY_PENALTY,\n",
        "            'presence_penalty': _PRESENCE_PENALTY\n",
        "        },\n",
        "        'Input Prompt': row['sentence_eng_Latn'],\n",
        "        'translated_text': translated_prompt,\n",
        "        'id': int(row['id']),\n",
        "    }\n",
        "    print(output_data)\n",
        "\n",
        "    with open(f\"{output_dir}/{lang}_translations_\"+str(i)+\".jsonl\", 'w') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "in22_dataset = load_dataset(\"ai4bharat/IN22-Conv\",\"eng_Latn-mar_Deva\")\n",
        "df = pd.DataFrame(in22_dataset[\"conv\"])\n",
        "lang = \"mar\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "    process_translation(i, df.iloc[i])\n",
        "# Run translations and gather results\n",
        "#Parallel(n_jobs=2)(delayed(process_translation)(i, row) for i, row in tqdm(df.head(3).iterrows()))\n",
        "\n",
        "# Write all results to a single JSON file with shared metadata\n",
        "\n",
        "## Run the script with the following command:\n",
        "# python generate_rag.py --lang=asm\n",
        "# python generate_rag.py --lang=ben\n",
        "# python generate_rag.py --lang=guj\n",
        "# python generate_rag.py --lang=hin\n",
        "# python generate_rag.py --lang=kan\n",
        "# python generate_rag.py --lang=mal\n",
        "# python generate_rag.py --lang=mar\n",
        "# python generate_rag.py --lang=nep\n",
        "# python generate_rag.py --lang=ori\n",
        "# python generate_rag.py --lang=pan\n",
        "# python generate_rag.py --lang=san\n",
        "# python generate_rag.py --lang=tam\n",
        "# python generate_rag.py --lang=tel\n",
        "# python generate_rag.py --lang=urd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RC-T1_m2Bhv"
      },
      "outputs": [],
      "source": [
        "'''import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from joblib import Parallel, delayed\n",
        "from openai import APIStatusError, OpenAI\n",
        "import threading\n",
        "\n",
        "# Constants and language mappings remain the same\n",
        "_TEMPERATURE = 0\n",
        "_MAX_TOKENS = 16384\n",
        "_TOP_P = 1\n",
        "_TOP_K = 64\n",
        "_FREQUENCY_PENALTY = 0\n",
        "_PRESENCE_PENALTY = 0\n",
        "\n",
        "_FLORES_MAP = {\n",
        "    \"Assamese\": \"asm_Beng\",\n",
        "    \"Bengali\": \"ben_Beng\",\n",
        "    \"Gujarati\": \"guj_Gujr\",\n",
        "    \"Hindi\": \"hin_Deva\",\n",
        "    \"Kannada\": \"kan_Knda\",\n",
        "    \"Malayalam\": \"mal_Mlym\",\n",
        "    \"Marathi\": \"mar_Deva\",\n",
        "    \"Nepali\": \"npi_Deva\",\n",
        "    \"Odia\": \"ory_Orya\",\n",
        "    \"Punjabi\": \"pan_Guru\",\n",
        "    \"Sanskrit\": \"san_Deva\",\n",
        "    \"Tamil\": \"tam_Taml\",\n",
        "    \"Telugu\": \"tel_Telu\",\n",
        "    \"Urdu\": \"urd_Arab\"\n",
        "}\n",
        "\n",
        "_LANG_MAP = {\n",
        "    \"asm\": \"Assamese\",\n",
        "    \"ben\": \"Bengali\",\n",
        "    \"guj\": \"Gujarati\",\n",
        "    \"hin\": \"Hindi\",\n",
        "    \"kan\": \"Kannada\",\n",
        "    \"mal\": \"Malayalam\",\n",
        "    \"mar\": \"Marathi\",\n",
        "    \"nep\": \"Nepali\",\n",
        "    \"ori\": \"Odia\",\n",
        "    \"pan\": \"Punjabi\",\n",
        "    \"san\": \"Sanskrit\",\n",
        "    \"tam\": \"Tamil\",\n",
        "    \"tel\": \"Telugu\",\n",
        "    \"urd\": \"Urdu\"\n",
        "}\n",
        "\n",
        "# Initialize the API client globally\n",
        "llama = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"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\"  # Replace with your actual API key\n",
        ")\n",
        "\n",
        "# Translation function remains the same\n",
        "def translate_text(llama_client, system_prompt, task_prompt, text):\n",
        "    \"\"\"Helper function to make a translation API call for a single text input.\"\"\"\n",
        "    _TRANSLATE_MODEL_INPUT = (\n",
        "        f\"{task_prompt}\"\n",
        "        f\"Input Text: {text}\\n\"\n",
        "        f\"Translated Text: \\n\"\n",
        "    )\n",
        "    try:\n",
        "        response = llama_client.chat.completions.create(\n",
        "            model=\"meta/llama-3.1-405b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": _TRANSLATE_MODEL_INPUT}\n",
        "            ],\n",
        "            temperature=0,\n",
        "            max_tokens=4096,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except APIStatusError as e:\n",
        "        if e.status_code in {402, 401}:\n",
        "            print(\"Issue with the API key, skipping...\")\n",
        "            return \"\"\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(\"Generation error encountered:\", e)\n",
        "        return \"\"\n",
        "\n",
        "# Modify process_translation to accept the llama client as a parameter\n",
        "def process_translation(args):\n",
        "    i, row, lang = args\n",
        "    target_lang = _LANG_MAP[lang]\n",
        "\n",
        "    _MT_SYSTEM_PROMPT = f\"You are a machine translation model. Please help in translating the following text to {target_lang}.\"\n",
        "    _MT_TASK_PROMPT = (\n",
        "        f\"Translate the following English text to {target_lang}. Follow these guidelines:\\n\\n\"\n",
        "        \"1. **Accuracy:** Translate the text as accurately as possible, ensuring no information is missed.\\n\"\n",
        "        \"2. **Tone and Naturalness:** Maintain the original tone and ensure the translation sounds natural.\\n\"\n",
        "        \"3. **Meaningful Restructuring:** Do not translate word-for-word. Restructure sentences if needed.\\n\"\n",
        "        \"4. **Do not Translate the numericals\\n\"\n",
        "        \"5. **Do not provide any explanations.\\n\\n\"\n",
        "        \"Ensure that the translated text is clear, coherent, and grammatically correct.\\n\\n\"\n",
        "    )\n",
        "\n",
        "    # Translate the input text\n",
        "    translated_prompt = translate_text(llama, _MT_SYSTEM_PROMPT, _MT_TASK_PROMPT, row['sentence_eng_Latn'])\n",
        "\n",
        "    output_dir = f\"/content/IndicTrans2/{lang}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_data = {\n",
        "        'source_lang': 'eng_Latn',\n",
        "        'target_lang': _FLORES_MAP[_LANG_MAP[lang]],\n",
        "        'meta': {\n",
        "            'model_name': 'Meta-Llama-3.1-405B-Instruct-FP32',\n",
        "            'temperature': _TEMPERATURE,\n",
        "            'max_tokens': _MAX_TOKENS,\n",
        "            'top_p': _TOP_P,\n",
        "            'frequency_penalty': _FREQUENCY_PENALTY,\n",
        "            'presence_penalty': _PRESENCE_PENALTY\n",
        "        },\n",
        "        'Input Prompt': row['sentence_eng_Latn'],\n",
        "        'translated_text': translated_prompt,\n",
        "        'id': int(row['id']),\n",
        "    }\n",
        "\n",
        "    # Save the result to a JSONL file\n",
        "    with open(f\"{output_dir}/{lang}_translations_{i}.jsonl\", 'w', encoding='utf-8') as f:\n",
        "        json.dump(output_data, f, ensure_ascii=False)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the dataset\n",
        "    in22_dataset = load_dataset(\"ai4bharat/IN22-Conv\", \"eng_Latn-asm_Beng\")\n",
        "    df = pd.DataFrame(in22_dataset[\"conv\"])\n",
        "    lang = \"asm\"  # Set your target language code here\n",
        "\n",
        "    # Prepare arguments for parallel processing\n",
        "    tasks = [(i, row, lang) for i, row in df.iterrows()]\n",
        "\n",
        "    # Set the number of jobs (adjust based on your environment)\n",
        "    num_jobs = 5  # You can set this to 5 or 10 as per your requirement\n",
        "\n",
        "    # Run translations in parallel\n",
        "    Parallel(n_jobs=num_jobs)(\n",
        "        delayed(process_translation)(args) for args in tqdm(tasks)\n",
        "    )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApelpHCPy1ZN"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#files.download(\"/content/in22-translations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26TbApnvJf6f"
      },
      "outputs": [],
      "source": [
        "#in22_dataset = load_dataset(\"ai4bharat/IN22-Conv\",\"eng_Latn-asm_Beng\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrpqhGm-Xxch"
      },
      "outputs": [],
      "source": [
        "#in22_dataset = load_dataset(\"ai4bharat/IN22-Conv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ll_aQiZMvC5"
      },
      "outputs": [],
      "source": [
        "#df.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTEz5jyRTitR"
      },
      "outputs": [],
      "source": [
        "#target_lang = \"Bengali\"\n",
        "#_MT_SYSTEM_PROMPT = f\"You are a machine translation model. Please help in translating the following text to {target_lang}.\"\n",
        "#_MT_TASK_PROMPT = (\n",
        " '''   f\"Translate the following English text to {target_lang}. Follow these guidelines:\\n\\n\"\n",
        "    \"1. **Accuracy:** Translate the text as accurately as possible, ensuring no information is missed.\\n\"\n",
        "    \"2. **Tone and Naturalness:** Maintain the original tone and ensure the translation sounds natural.\\n\"\n",
        "    \"3. **Meaningful Restructuring:** Do not translate word-for-word. Restructure sentences if needed.\\n\"\n",
        "    \"4. **Do not Translate the numericals\\n\"\n",
        "    \"5. **Do not provide any Explnations.\\n\\n\"\n",
        "    \"Ensure that the translated text is clear, coherent, and grammatically correct.\\n\\n\"\n",
        ")\n",
        "llama = OpenAI(\n",
        "        base_url = \"https://integrate.api.nvidia.com/v1\",\n",
        "        api_key = \"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\"\n",
        "    )\n",
        "# Translate Prompt and Answer\n",
        "translated_prompt = translate_text(llama, _MT_SYSTEM_PROMPT, _MT_TASK_PROMPT, df.iloc[10]['sentence_eng_Latn'])'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG6XVqhhULQd"
      },
      "outputs": [],
      "source": [
        "###translated_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4uMzh65JjB9"
      },
      "outputs": [],
      "source": [
        "#in22_dataset[\"conv\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lhKFjrwKBAS"
      },
      "outputs": [],
      "source": [
        "#! lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9melFL_YPh6S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05dfdbbb459b4a769b8c14adbddbc459": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b25f2b51c72418b9825dc755fd3f69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55ce8764eb5a43868dc347631da7e3fe",
              "IPY_MODEL_8fbe99c86e01460eb4391e4431707044",
              "IPY_MODEL_e975ce6090ed417486dd5a9dc0d282f1"
            ],
            "layout": "IPY_MODEL_fb50ff166fa143a5af98f2eeb1e81be1"
          }
        },
        "215f6b2ac1c64db38cdf79ec670cdb1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba04c64cf77437185715353a18ea117": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ce8764eb5a43868dc347631da7e3fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215f6b2ac1c64db38cdf79ec670cdb1c",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba04c64cf77437185715353a18ea117",
            "value": "Generating conv split: "
          }
        },
        "6a43337d5e9849f380b8a9ab14673247": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78e3ab571dfb4013ab417358d3a4b12c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8fbe99c86e01460eb4391e4431707044": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78e3ab571dfb4013ab417358d3a4b12c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a43337d5e9849f380b8a9ab14673247",
            "value": 1
          }
        },
        "9ace13b5690f4d2b89e304c496c680c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e975ce6090ed417486dd5a9dc0d282f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ace13b5690f4d2b89e304c496c680c4",
            "placeholder": "​",
            "style": "IPY_MODEL_05dfdbbb459b4a769b8c14adbddbc459",
            "value": " 1503/0 [00:00&lt;00:00, 5757.76 examples/s]"
          }
        },
        "fb50ff166fa143a5af98f2eeb1e81be1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}